{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "toknizer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sharhp21/ChatBot_SW_Quest/blob/master/toknizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fI72EI5BEbV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pprint\n",
        "## 사용예\n",
        "## pprint.pprint(\"value\", width=500)\n",
        "\n",
        "sample_data = \"Don't be a jerk is a fundamental rule of all social spaces. Every other policy for getting along with others is a special case of this rule.\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOOfT01mDNu2",
        "colab_type": "text"
      },
      "source": [
        "** nltk 연습하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsLLNJynDMJu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_file(fileName):\n",
        "    file = open(fileName)\n",
        "    data = file.read()\n",
        "    file.close()\n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKcIZJBRF9nZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jT1S0q4lDQ4G",
        "colab_type": "text"
      },
      "source": [
        "** konlpy 연습하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70EGV5gA0NHl",
        "colab_type": "code",
        "outputId": "d1b1e2e8-3e81-4548-f88f-32b1f6601984",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 613
        }
      },
      "source": [
        "!pip install konlpy"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting konlpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/0e/f385566fec837c0b83f216b2da65db9997b35dd675e107752005b7d392b1/konlpy-0.5.2-py2.py3-none-any.whl (19.4MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4MB 161kB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.2.6)\n",
            "Collecting beautifulsoup4==4.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 11.4MB/s \n",
            "\u001b[?25hCollecting tweepy>=3.7.0\n",
            "  Downloading https://files.pythonhosted.org/packages/36/1b/2bd38043d22ade352fc3d3902cf30ce0e2f4bf285be3b304a2782a767aec/tweepy-3.8.0-py2.py3-none-any.whl\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/c9/dc/45cdef1b4d119eb96316b3117e6d5708a08029992b2fee2c143c7a0a5cc5/colorama-0.4.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.17.5)\n",
            "Collecting JPype1>=0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/04/90/a94a55a58edfd67360fef85894bfb136a2c28b2cc7227d3a44dc508d5900/JPype1-0.7.1-cp36-cp36m-manylinux1_x86_64.whl (2.3MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3MB 53.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.12.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: requests>=2.11.1 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (2.21.0)\n",
            "Requirement already satisfied: PySocks>=1.5.7 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.11.1->tweepy>=3.7.0->konlpy) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.11.1->tweepy>=3.7.0->konlpy) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Installing collected packages: beautifulsoup4, tweepy, colorama, JPype1, konlpy\n",
            "  Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "  Found existing installation: tweepy 3.6.0\n",
            "    Uninstalling tweepy-3.6.0:\n",
            "      Successfully uninstalled tweepy-3.6.0\n",
            "Successfully installed JPype1-0.7.1 beautifulsoup4-4.6.0 colorama-0.4.3 konlpy-0.5.2 tweepy-3.8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgaVkLjG0Rxg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from konlpy.tag import Okt\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8QrwwQb0YkO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "okt = Okt()\n",
        "text = \"자연어는 처리가 필요합니다. 전처리가 중요하지요. 철수가 전처리를 합니다.\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0_cKrIa0Yv4",
        "colab_type": "code",
        "outputId": "4928441c-9697-45a4-ad0a-ba9814e7b69e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        }
      },
      "source": [
        "start = time.time()\n",
        "print(okt.morphs(text))\n",
        "print('\\n')\n",
        "print(okt.pos(text))\n",
        "print('\\n')\n",
        "print(okt.nouns(text))\n",
        "print(time.time() - start)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['자연어', '는', '처리', '가', '필요합니다', '.', '전', '처리', '가', '중요하지요', '.', '철수', '가', '전', '처리', '를', '합니다', '.']\n",
            "\n",
            "\n",
            "[('자연어', 'Noun'), ('는', 'Josa'), ('처리', 'Noun'), ('가', 'Josa'), ('필요합니다', 'Adjective'), ('.', 'Punctuation'), ('전', 'Modifier'), ('처리', 'Noun'), ('가', 'Josa'), ('중요하지요', 'Adjective'), ('.', 'Punctuation'), ('철수', 'Noun'), ('가', 'Josa'), ('전', 'Modifier'), ('처리', 'Noun'), ('를', 'Josa'), ('합니다', 'Verb'), ('.', 'Punctuation')]\n",
            "\n",
            "\n",
            "['자연어', '처리', '처리', '철수', '처리']\n",
            "6.144374370574951\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ir8zd1lM0Y4O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from konlpy.tag import Kkma"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyV778brAT-n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kkma = Kkma()\n",
        "text = \"자연어는 처리가 필요합니다. 전처리가 중요하지요. 철수가 전처리를 합니다.\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TH0-MAdAbiU",
        "colab_type": "code",
        "outputId": "3ee70dc9-28f5-41b3-ef3e-7b920fcecc2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        }
      },
      "source": [
        "start = time.time()\n",
        "print(kkma.morphs(text))\n",
        "print('\\n')\n",
        "print(kkma.pos(text))\n",
        "print('\\n')\n",
        "print(kkma.nouns(text))\n",
        "print(time.time() - start)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['자연어', '는', '처리', '가', '필요', '하', 'ㅂ니다', '.', '전처리', '가', '중요', '하', '지요', '.', '철수', '가', '전처리', '를', '하', 'ㅂ니다', '.']\n",
            "\n",
            "\n",
            "[('자연어', 'NNG'), ('는', 'JX'), ('처리', 'NNG'), ('가', 'JKS'), ('필요', 'NNG'), ('하', 'XSV'), ('ㅂ니다', 'EFN'), ('.', 'SF'), ('전처리', 'NNG'), ('가', 'JKS'), ('중요', 'NNG'), ('하', 'XSV'), ('지요', 'EFN'), ('.', 'SF'), ('철수', 'NNG'), ('가', 'JKS'), ('전처리', 'NNG'), ('를', 'JKO'), ('하', 'VV'), ('ㅂ니다', 'EFN'), ('.', 'SF')]\n",
            "\n",
            "\n",
            "['자연어', '처리', '필요', '전처리', '중요', '철수']\n",
            "11.072253942489624\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1M766uwKD5bj",
        "colab_type": "text"
      },
      "source": [
        "** lemmatize 어간추출"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhyImqMK-5GA",
        "colab_type": "code",
        "outputId": "866332e1-c54b-4893-a3cb-bea1fdc61d27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "import nltk \n",
        "from nltk.stem import WordNetLemmatizer \n",
        "\n",
        "nltk.download('wordnet')\n",
        "\n",
        "n=WordNetLemmatizer() \n",
        "words=['lives', 'started', 'has', 'seen', 'eaten'] \n",
        "\n",
        "[n.lemmatize(w) for w in words]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['life', 'started', 'ha', 'seen', 'eaten']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2B8LkFvAcVd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk \n",
        "from nltk.stem import WordNetLemmatizer \n",
        "n=WordNetLemmatizer() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3bX-_jQAmcW",
        "colab_type": "code",
        "outputId": "cfd789a1-624e-44ff-ebb3-89359e2454cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "n.lemmatize('has', 'v')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'have'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zu1IRhVwAq8i",
        "colab_type": "code",
        "outputId": "ec7bd62f-c970-4916-8566-418f9b4825a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "n.lemmatize('started', 'v')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'start'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPnTcXgGktXs",
        "colab_type": "code",
        "outputId": "07b00afb-e873-46ee-b4c6-63dfeb28c098",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        }
      },
      "source": [
        "import nltk \n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "print('\\n')\n",
        "pprint.pprint(stopwords.words('english')[:50], width=500)\n",
        "print('\\n')\n",
        "print(len(stopwords.words('english')))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "\n",
            "\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be']\n",
            "\n",
            "\n",
            "179\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ca1_y0gUC_PD",
        "colab_type": "code",
        "outputId": "4dcbfc10-eaf4-4983-b21c-03361f7b9c65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "source": [
        "import nltk \n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize \n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "text = \"We have the ability to create personalized premium content across a wide range of verticals, with fitness being our first vertical\"\n",
        "stop_words = set(stopwords.words('english'))\n",
        "tokens = word_tokenize(text) \n",
        "result = [] \n",
        "for w in tokens : \n",
        "    if w not in stop_words: \n",
        "        result.append(w)\n",
        "    else:\n",
        "        print(w,end=', ')\n",
        "\n",
        "print('\\n')\n",
        "print(tokens)\n",
        "print('\\n')\n",
        "print(result)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "have, the, to, a, of, with, being, our, \n",
            "\n",
            "['We', 'have', 'the', 'ability', 'to', 'create', 'personalized', 'premium', 'content', 'across', 'a', 'wide', 'range', 'of', 'verticals', ',', 'with', 'fitness', 'being', 'our', 'first', 'vertical']\n",
            "\n",
            "\n",
            "['We', 'ability', 'create', 'personalized', 'premium', 'content', 'across', 'wide', 'range', 'verticals', ',', 'fitness', 'first', 'vertical']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKffZXikDgjC",
        "colab_type": "code",
        "outputId": "3c9d3ac5-8d56-43fe-b046-7bd1d3a2982d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "stop_words"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'a',\n",
              " 'about',\n",
              " 'above',\n",
              " 'after',\n",
              " 'again',\n",
              " 'against',\n",
              " 'ain',\n",
              " 'all',\n",
              " 'am',\n",
              " 'an',\n",
              " 'and',\n",
              " 'any',\n",
              " 'are',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'as',\n",
              " 'at',\n",
              " 'be',\n",
              " 'because',\n",
              " 'been',\n",
              " 'before',\n",
              " 'being',\n",
              " 'below',\n",
              " 'between',\n",
              " 'both',\n",
              " 'but',\n",
              " 'by',\n",
              " 'can',\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'd',\n",
              " 'did',\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'do',\n",
              " 'does',\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'doing',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'down',\n",
              " 'during',\n",
              " 'each',\n",
              " 'few',\n",
              " 'for',\n",
              " 'from',\n",
              " 'further',\n",
              " 'had',\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'has',\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'have',\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'having',\n",
              " 'he',\n",
              " 'her',\n",
              " 'here',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'him',\n",
              " 'himself',\n",
              " 'his',\n",
              " 'how',\n",
              " 'i',\n",
              " 'if',\n",
              " 'in',\n",
              " 'into',\n",
              " 'is',\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'just',\n",
              " 'll',\n",
              " 'm',\n",
              " 'ma',\n",
              " 'me',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'more',\n",
              " 'most',\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'my',\n",
              " 'myself',\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'now',\n",
              " 'o',\n",
              " 'of',\n",
              " 'off',\n",
              " 'on',\n",
              " 'once',\n",
              " 'only',\n",
              " 'or',\n",
              " 'other',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'out',\n",
              " 'over',\n",
              " 'own',\n",
              " 're',\n",
              " 's',\n",
              " 'same',\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'so',\n",
              " 'some',\n",
              " 'such',\n",
              " 't',\n",
              " 'than',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'the',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'them',\n",
              " 'themselves',\n",
              " 'then',\n",
              " 'there',\n",
              " 'these',\n",
              " 'they',\n",
              " 'this',\n",
              " 'those',\n",
              " 'through',\n",
              " 'to',\n",
              " 'too',\n",
              " 'under',\n",
              " 'until',\n",
              " 'up',\n",
              " 've',\n",
              " 'very',\n",
              " 'was',\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'we',\n",
              " 'were',\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'what',\n",
              " 'when',\n",
              " 'where',\n",
              " 'which',\n",
              " 'while',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'why',\n",
              " 'will',\n",
              " 'with',\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\",\n",
              " 'y',\n",
              " 'you',\n",
              " \"you'd\",\n",
              " \"you'll\",\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcDnOLsZAtHd",
        "colab_type": "code",
        "outputId": "27830ec5-bf8d-436d-98c5-ff2a3af6ef24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "source": [
        "import nltk \n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize \n",
        "\n",
        "text  = \"개발 중 혹은 배포된 소프트웨어에서 이미 발견한 버그의 상태 변화 과정을 추적하기 위한 소프트웨어다. 버그가 수정되었는지 여부, 수정 중이라면 현재 진행상황을 알 수 있고, 이미 알려진 버그의 누락을 막을 수 있으며, 버그 수정과 관련된 불필요한 커뮤니케이션 비용을 줄일 수 있다.\" \n",
        "stop_words = \"혹은 이미 위한 있다 있으며\"\n",
        "\n",
        "stop_words=stop_words.split(' ')\n",
        "tokens = word_tokenize(text) \n",
        "result = [] \n",
        "\n",
        "for w in tokens: \n",
        "    if w not in stop_words:\n",
        "        result.append(w) \n",
        "\n",
        "print('\\n')\n",
        "print(tokens)\n",
        "print('\\n') \n",
        "print(result)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "['개발', '중', '혹은', '배포된', '소프트웨어에서', '이미', '발견한', '버그의', '상태', '변화', '과정을', '추적하기', '위한', '소프트웨어다', '.', '버그가', '수정되었는지', '여부', ',', '수정', '중이라면', '현재', '진행상황을', '알', '수', '있고', ',', '이미', '알려진', '버그의', '누락을', '막을', '수', '있으며', ',', '버그', '수정과', '관련된', '불필요한', '커뮤니케이션', '비용을', '줄일', '수', '있다', '.']\n",
            "\n",
            "\n",
            "['개발', '중', '배포된', '소프트웨어에서', '발견한', '버그의', '상태', '변화', '과정을', '추적하기', '소프트웨어다', '.', '버그가', '수정되었는지', '여부', ',', '수정', '중이라면', '현재', '진행상황을', '알', '수', '있고', ',', '알려진', '버그의', '누락을', '막을', '수', ',', '버그', '수정과', '관련된', '불필요한', '커뮤니케이션', '비용을', '줄일', '수', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}